# Scenario: Rolling Restart
#
# Goal: Test service availability and correctness during planned maintenance.
#
# Description:
# This scenario simulates a rolling restart, where each node is taken down and
# brought back up in sequence. This is a common operational procedure for deploying
# updates. The test verifies that the cluster maintains quorum and remains
# available throughout the process. It also tests a riskier "overlapping"
# restart where multiple nodes are down simultaneously.

name = "rolling_restart_test"
seed = 2004
topology = "FullMesh"

[initial]
nodes = 5
proto = 1  # Raft protocol

stop_at = 20_000_000_000  # 20 seconds

# --- Phase 1: Stable operation (0-2s) ---
# Allow the cluster to stabilize before starting the maintenance.

# --- Phase 2: Slow rolling restart (2s-12s) ---
# Restart each node one by one, with a 2-second interval between restarts
# and a 1-second downtime for each node. This is a safe procedure.
[[directives]]
At = [2_000_000_000, { Crash = { node = 0, duration = 1_000_000_000 } }]
[[directives]]
At = [4_000_000_000, { Crash = { node = 1, duration = 1_000_000_000 } }]
[[directives]]
At = [6_000_000_000, { Crash = { node = 2, duration = 1_000_000_000 } }]
[[directives]]
At = [8_000_000_000, { Crash = { node = 3, duration = 1_000_000_000 } }]
[[directives]]
At = [10_000_000_000, { Crash = { node = 4, duration = 1_000_000_000 } }]

# --- Phase 3: Test during rolling restart ---
# Send requests during the restart to ensure the cluster remains available.
[[directives]]
At = [3_000_000_000, { BroadcastBytes = { payload_hex = "524f4c4c494e475f524553544152545f31", proto_tag = 1 } }]
[[directives]]
At = [5_000_000_000, { BroadcastBytes = { payload_hex = "524f4c4c494e475f524553544152545f32", proto_tag = 1 } }]

# --- Phase 4: Fast rolling restart (12s-17s) ---
# A more aggressive restart with shorter intervals and downtime.
[[directives]]
At = [12_000_000_000, { Crash = { node = 0, duration = 500_000_000 } }]
[[directives]]
At = [13_000_000_000, { Crash = { node = 1, duration = 500_000_000 } }]
# ... and so on for all nodes.

# --- Phase 5: Overlapping restart (17s-19s) ---
# A high-risk scenario where two nodes are down at the same time. In a 5-node
# cluster, this means quorum is maintained (3/5 nodes are up), but it tests
# the system's behavior at the edge of availability.
[[directives]]
At = [17_000_000_000, { Crash = { node = 0, duration = 1_500_000_000 } }]
[[directives]]
At = [17_500_000_000, { Crash = { node = 1, duration = 1_500_000_000 } }]
[[directives]]
At = [18_000_000_000, { BroadcastBytes = { payload_hex = "4f5645524c41505f524553544152545f54455354", proto_tag = 1 } }]

# --- Phase 6: Recovery verification (19s-20s) ---
# Verify the cluster is healthy after all restarts are complete.
[[directives]]
At = [19_500_000_000, { BroadcastBytes = { payload_hex = "46494e414c5f564552494649434154494f4e", proto_tag = 1 } }]
